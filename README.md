## Протокол встречи из транскрипта встречи (SGR)

CLI‑скрипт для генерации протокола совещания из текстового транскрипта по технике Schema‑Guided Reasoning (один вызов LLM с заполнением схемы).

### Требования
- Установленный [uv](https://github.com/astral-sh/uv?tab=readme-ov-file#installation) (запуск и управление зависимостями в Python-проектах)

### Настройка
Создайте файл `.env` в корне проекта на основе `.env.example` и задайте параметры провайдеров LLM, которых планируете использовать.
Достаточно настроить одного из провайдеров (локальный или облачный). Указывайте совместимый с OpenAI API endpoint и ключ, модель — идентификатор модели.

```env
# Local/secure LLM provided
API_BASE_LOCAL=
API_KEY_LOCAL=
MODEL_LOCAL=

# Cloud LLM provider
API_BASE_CLOUD=
API_KEY_CLOUD=
MODEL_CLOUD=
```

### Запуск
Базовый сценарий (по умолчанию - локальный провайдер):

```bash
uv run main.py path\to\transcript.txt
```

Использовать облачного провайдера:

```bash
uv run main.py path\to\transcript.txt --online
```

Переопределить модель для текущего провайдера (по умолчанию - модель, указанная в `.env`):

```bash
uv run main.py path\to\transcript.txt -m model-id
```

Задать температуру семплирования (по умолчанию 0.15):

```bash
uv run main.py path\to\transcript.txt -t 0.2
```

### Температура: влияние и рекомендации
- **Низкая (0.0–0.2)**: более детерминированный и фактологичный вывод. Рекомендуется для протоколов.
- **Средняя (0.2–0.4)**: немного больше вариативности формулировок.
- **Высокая (0.5+)**: креативнее, но выше риск отклонений от исходного текста — обычно не нужно для протоколов.

По умолчанию используется **0.15**.

### Результат
- Итоговый протокол сохраняется рядом с исходным файлом транскрипта под именем вида:
  `<basename>-<тема_встречи>-<model>[<время генерации>].txt`.



